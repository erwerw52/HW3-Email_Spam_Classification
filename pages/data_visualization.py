"""
Data Visualization é é¢
æ•¸æ“šå¯è¦–åŒ–å’Œæ¢ç´¢æ€§æ•¸æ“šåˆ†æ
"""

import streamlit as st
import pandas as pd
import numpy as np
from typing import Any

def show_page(data: pd.DataFrame, data_loader: Any, preprocessor: Any, visualizer: Any):
    """é¡¯ç¤º Data Visualization é é¢"""
    
    st.title("ğŸ“ˆ Data Visualization")
    st.markdown("æ¢ç´¢å’Œå¯è¦–åŒ–åƒåœ¾éƒµä»¶æ•¸æ“šé›†çš„ç‰¹å¾µå’Œåˆ†å¸ƒ")
    
    if data.empty:
        st.error("ç„¡æ³•è¼‰å…¥æ•¸æ“šé›†")
        return
    
    # æ•¸æ“šæ¦‚è¦½
    st.subheader("ğŸ“Š æ•¸æ“šé›†æ¦‚è¦½")
    
    dataset_info = data_loader.get_dataset_info()
    
    # å‰µå»ºæŒ‡æ¨™å¡ç‰‡
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "ç¸½éƒµä»¶æ•¸", 
            f"{dataset_info.get('total_messages', 0):,}",
            help="æ•¸æ“šé›†ä¸­çš„ç¸½éƒµä»¶æ•¸é‡"
        )
    
    with col2:
        st.metric(
            "åƒåœ¾éƒµä»¶", 
            f"{dataset_info.get('spam_count', 0):,}",
            delta=f"{dataset_info.get('spam_percentage', 0):.1f}%",
            help="åƒåœ¾éƒµä»¶æ•¸é‡å’Œæ¯”ä¾‹"
        )
    
    with col3:
        st.metric(
            "æ­£å¸¸éƒµä»¶", 
            f"{dataset_info.get('ham_count', 0):,}",
            delta=f"{dataset_info.get('ham_percentage', 0):.1f}%",
            help="æ­£å¸¸éƒµä»¶æ•¸é‡å’Œæ¯”ä¾‹"
        )
    
    with col4:
        st.metric(
            "å¹³å‡é•·åº¦", 
            f"{dataset_info.get('avg_message_length', 0):.0f}",
            help="éƒµä»¶å¹³å‡å­—ç¬¦é•·åº¦"
        )
    
    # é¡åˆ¥åˆ†å¸ƒåœ–
    st.subheader("ğŸ“Š é¡åˆ¥åˆ†å¸ƒ")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        class_dist_fig = visualizer.create_class_distribution_chart(data)
        st.plotly_chart(class_dist_fig, use_container_width=True)
    
    with col2:
        # é¤…åœ–
        import plotly.express as px
        class_counts = data['label'].value_counts()
        pie_fig = px.pie(
            values=class_counts.values,
            names=class_counts.index,
            title="é¡åˆ¥æ¯”ä¾‹åˆ†å¸ƒ",
            color_discrete_map={'ham': '#4ECDC4', 'spam': '#FF6B6B'}
        )
        st.plotly_chart(pie_fig, use_container_width=True)
    
    # æ–‡æœ¬é•·åº¦åˆ†æ
    st.subheader("ğŸ“ æ–‡æœ¬é•·åº¦åˆ†æ")
    
    text_length_fig = visualizer.create_text_length_distribution(data)
    st.plotly_chart(text_length_fig, use_container_width=True)
    
    # æ–‡æœ¬é•·åº¦çµ±è¨ˆ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**åƒåœ¾éƒµä»¶æ–‡æœ¬çµ±è¨ˆ**")
        spam_data = data[data['label'] == 'spam']
        spam_lengths = spam_data['text'].str.len()
        
        st.write(f"å¹³å‡é•·åº¦: {spam_lengths.mean():.0f} å­—ç¬¦")
        st.write(f"ä¸­ä½æ•¸é•·åº¦: {spam_lengths.median():.0f} å­—ç¬¦")
        st.write(f"æœ€é•·: {spam_lengths.max()} å­—ç¬¦")
        st.write(f"æœ€çŸ­: {spam_lengths.min()} å­—ç¬¦")
    
    with col2:
        st.markdown("**æ­£å¸¸éƒµä»¶æ–‡æœ¬çµ±è¨ˆ**")
        ham_data = data[data['label'] == 'ham']
        ham_lengths = ham_data['text'].str.len()
        
        st.write(f"å¹³å‡é•·åº¦: {ham_lengths.mean():.0f} å­—ç¬¦")
        st.write(f"ä¸­ä½æ•¸é•·åº¦: {ham_lengths.median():.0f} å­—ç¬¦")
        st.write(f"æœ€é•·: {ham_lengths.max()} å­—ç¬¦")
        st.write(f"æœ€çŸ­: {ham_lengths.min()} å­—ç¬¦")
    
    # Top Tokens åˆ†æ
    st.subheader("ğŸ”¤ Top Tokens åˆ†æ")
    
    # é è™•ç†æ–‡æœ¬ç”¨æ–¼åˆ†æ
    with st.spinner("æ­£åœ¨åˆ†ææ–‡æœ¬ç‰¹å¾µ..."):
        processed_texts = []
        labels = []
        
        for _, row in data.iterrows():
            processed_text = preprocessor.preprocess_text(row['text'])
            processed_texts.append(processed_text)
            labels.append(row['label'])
        
        # ç²å– Top Tokens
        top_tokens = preprocessor.get_top_tokens(processed_texts, labels, n_tokens=20)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**åƒåœ¾éƒµä»¶ Top Tokens**")
        if top_tokens['spam']:
            spam_tokens_fig = visualizer.create_top_tokens_chart(
                top_tokens['spam'], 
                "åƒåœ¾éƒµä»¶é«˜é »è©å½™", 
                visualizer.color_palette['spam']
            )
            st.plotly_chart(spam_tokens_fig, use_container_width=True)
        else:
            st.warning("ç„¡æ³•ç²å–åƒåœ¾éƒµä»¶è©å½™æ•¸æ“š")
    
    with col2:
        st.markdown("**æ­£å¸¸éƒµä»¶ Top Tokens**")
        if top_tokens['ham']:
            ham_tokens_fig = visualizer.create_top_tokens_chart(
                top_tokens['ham'], 
                "æ­£å¸¸éƒµä»¶é«˜é »è©å½™", 
                visualizer.color_palette['ham']
            )
            st.plotly_chart(ham_tokens_fig, use_container_width=True)
        else:
            st.warning("ç„¡æ³•ç²å–æ­£å¸¸éƒµä»¶è©å½™æ•¸æ“š")
    
    # è©é›²åœ–
    st.subheader("â˜ï¸ è©é›²åœ–")
    
    wordcloud_type = st.radio(
        "é¸æ“‡è©é›²é¡å‹",
        ["åƒåœ¾éƒµä»¶", "æ­£å¸¸éƒµä»¶", "å…¨éƒ¨éƒµä»¶"],
        horizontal=True,
        help="é¸æ“‡è¦ç”Ÿæˆè©é›²çš„éƒµä»¶é¡å‹"
    )
    
    if wordcloud_type == "åƒåœ¾éƒµä»¶":
        spam_texts = ' '.join([text for text, label in zip(processed_texts, labels) if label == 'spam'])
        if spam_texts.strip():
            wordcloud_fig = visualizer.create_wordcloud(spam_texts, "åƒåœ¾éƒµä»¶è©é›²")
            st.pyplot(wordcloud_fig)
        else:
            st.warning("æ²’æœ‰è¶³å¤ çš„åƒåœ¾éƒµä»¶æ•¸æ“šç”Ÿæˆè©é›²")
    
    elif wordcloud_type == "æ­£å¸¸éƒµä»¶":
        ham_texts = ' '.join([text for text, label in zip(processed_texts, labels) if label == 'ham'])
        if ham_texts.strip():
            wordcloud_fig = visualizer.create_wordcloud(ham_texts, "æ­£å¸¸éƒµä»¶è©é›²")
            st.pyplot(wordcloud_fig)
        else:
            st.warning("æ²’æœ‰è¶³å¤ çš„æ­£å¸¸éƒµä»¶æ•¸æ“šç”Ÿæˆè©é›²")
    
    else:  # å…¨éƒ¨éƒµä»¶
        all_texts = ' '.join(processed_texts)
        if all_texts.strip():
            wordcloud_fig = visualizer.create_wordcloud(all_texts, "å…¨éƒ¨éƒµä»¶è©é›²")
            st.pyplot(wordcloud_fig)
        else:
            st.warning("æ²’æœ‰è¶³å¤ çš„æ•¸æ“šç”Ÿæˆè©é›²")
    
    # æ•¸æ“šæ¨£æœ¬å±•ç¤º
    st.subheader("ğŸ“‹ æ•¸æ“šæ¨£æœ¬")
    
    sample_type = st.selectbox(
        "é¸æ“‡è¦æŸ¥çœ‹çš„æ¨£æœ¬é¡å‹",
        ["å…¨éƒ¨", "åƒåœ¾éƒµä»¶", "æ­£å¸¸éƒµä»¶"],
        help="é¸æ“‡è¦å±•ç¤ºçš„éƒµä»¶é¡å‹"
    )
    
    sample_size = st.slider(
        "æ¨£æœ¬æ•¸é‡",
        min_value=5,
        max_value=50,
        value=10,
        help="é¸æ“‡è¦é¡¯ç¤ºçš„æ¨£æœ¬æ•¸é‡"
    )
    
    if sample_type == "å…¨éƒ¨":
        sample_data = data.sample(n=min(sample_size, len(data)))
    elif sample_type == "åƒåœ¾éƒµä»¶":
        spam_data = data[data['label'] == 'spam']
        sample_data = spam_data.sample(n=min(sample_size, len(spam_data)))
    else:  # æ­£å¸¸éƒµä»¶
        ham_data = data[data['label'] == 'ham']
        sample_data = ham_data.sample(n=min(sample_size, len(ham_data)))
    
    # é¡¯ç¤ºæ¨£æœ¬
    for idx, row in sample_data.iterrows():
        with st.expander(f"{row['label'].upper()} - {row['text'][:50]}..."):
            st.write(f"**æ¨™ç±¤**: {row['label']}")
            st.write(f"**é•·åº¦**: {len(row['text'])} å­—ç¬¦")
            st.write(f"**å…§å®¹**: {row['text']}")
    
    # æ•¸æ“šè³ªé‡åˆ†æ
    st.subheader("ğŸ” æ•¸æ“šè³ªé‡åˆ†æ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**æ•¸æ“šå®Œæ•´æ€§**")
        
        # æª¢æŸ¥ç¼ºå¤±å€¼
        missing_labels = data['label'].isnull().sum()
        missing_texts = data['text'].isnull().sum()
        
        st.write(f"ç¼ºå¤±æ¨™ç±¤: {missing_labels}")
        st.write(f"ç¼ºå¤±æ–‡æœ¬: {missing_texts}")
        
        # æª¢æŸ¥ç©ºæ–‡æœ¬
        empty_texts = (data['text'].str.strip() == '').sum()
        st.write(f"ç©ºæ–‡æœ¬: {empty_texts}")
        
        # æª¢æŸ¥é‡è¤‡
        duplicates = data.duplicated().sum()
        st.write(f"é‡è¤‡è¨˜éŒ„: {duplicates}")
    
    with col2:
        st.markdown("**æ¨™ç±¤åˆ†å¸ƒå¹³è¡¡æ€§**")
        
        spam_ratio = dataset_info.get('spam_percentage', 0) / 100
        ham_ratio = dataset_info.get('ham_percentage', 0) / 100
        
        # è¨ˆç®—ä¸å¹³è¡¡ç¨‹åº¦
        imbalance_ratio = max(spam_ratio, ham_ratio) / min(spam_ratio, ham_ratio)
        
        st.write(f"åƒåœ¾éƒµä»¶æ¯”ä¾‹: {spam_ratio:.1%}")
        st.write(f"æ­£å¸¸éƒµä»¶æ¯”ä¾‹: {ham_ratio:.1%}")
        st.write(f"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
        
        if imbalance_ratio > 3:
            st.warning("âš ï¸ æ•¸æ“šé›†å­˜åœ¨åš´é‡ä¸å¹³è¡¡")
        elif imbalance_ratio > 2:
            st.info("â„¹ï¸ æ•¸æ“šé›†å­˜åœ¨è¼•å¾®ä¸å¹³è¡¡")
        else:
            st.success("âœ… æ•¸æ“šé›†ç›¸å°å¹³è¡¡")
    
    # å°å‡ºåŠŸèƒ½
    st.subheader("ğŸ’¾ æ•¸æ“šå°å‡º")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("å°å‡ºæ•¸æ“šçµ±è¨ˆå ±å‘Š", use_container_width=True):
            # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
            report = generate_data_report(data, dataset_info, top_tokens)
            st.download_button(
                label="ä¸‹è¼‰çµ±è¨ˆå ±å‘Š (TXT)",
                data=report,
                file_name="spam_data_report.txt",
                mime="text/plain"
            )
    
    with col2:
        if st.button("å°å‡ºæ¨£æœ¬æ•¸æ“š", use_container_width=True):
            # å°å‡º CSV
            csv_data = data.to_csv(index=False)
            st.download_button(
                label="ä¸‹è¼‰æ•¸æ“šé›† (CSV)",
                data=csv_data,
                file_name="spam_dataset.csv",
                mime="text/csv"
            )

def generate_data_report(data: pd.DataFrame, dataset_info: dict, top_tokens: dict) -> str:
    """ç”Ÿæˆæ•¸æ“šçµ±è¨ˆå ±å‘Š"""
    
    report = f"""
åƒåœ¾éƒµä»¶æ•¸æ“šé›†çµ±è¨ˆå ±å‘Š
===================

æ•¸æ“šé›†åŸºæœ¬ä¿¡æ¯:
- ç¸½éƒµä»¶æ•¸: {dataset_info.get('total_messages', 0):,}
- åƒåœ¾éƒµä»¶æ•¸: {dataset_info.get('spam_count', 0):,} ({dataset_info.get('spam_percentage', 0):.1f}%)
- æ­£å¸¸éƒµä»¶æ•¸: {dataset_info.get('ham_count', 0):,} ({dataset_info.get('ham_percentage', 0):.1f}%)

æ–‡æœ¬é•·åº¦çµ±è¨ˆ:
- å¹³å‡é•·åº¦: {dataset_info.get('avg_message_length', 0):.0f} å­—ç¬¦
- æœ€å¤§é•·åº¦: {dataset_info.get('max_message_length', 0)} å­—ç¬¦
- æœ€å°é•·åº¦: {dataset_info.get('min_message_length', 0)} å­—ç¬¦

åƒåœ¾éƒµä»¶ Top 10 è©å½™:
"""
    
    for i, (word, count) in enumerate(top_tokens.get('spam', [])[:10]):
        report += f"{i+1:2d}. {word}: {count}\n"
    
    report += "\næ­£å¸¸éƒµä»¶ Top 10 è©å½™:\n"
    
    for i, (word, count) in enumerate(top_tokens.get('ham', [])[:10]):
        report += f"{i+1:2d}. {word}: {count}\n"
    
    report += f"\nå ±å‘Šç”Ÿæˆæ™‚é–“: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    
    return report