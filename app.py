"""
åƒåœ¾éƒµä»¶åˆ†é¡ç³»çµ± - Phase 1
ä¸»æ‡‰ç”¨æ–‡ä»¶
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime

# å°å…¥è‡ªå®šç¾©æ¨¡å¡Š
from src.config import APP_CONFIG
from src.data_loader import DataLoader
from src.preprocessing import TextPreprocessor
from src.classifier import MultiModelClassifier
from src.visualizations import Visualizer

# å°å…¥é é¢æ¨¡å¡Š
from pages import live_inference, model_performance, data_visualization

def main():
    """ä¸»æ‡‰ç”¨å‡½æ•¸"""
    
    # é é¢é…ç½®
    st.set_page_config(
        page_title=APP_CONFIG['title'],
        page_icon=APP_CONFIG['icon'],
        layout=APP_CONFIG['layout'],
        initial_sidebar_state="expanded"
    )
    
    # åˆå§‹åŒ– session stateï¼ˆå¿…é ˆåœ¨æœ€é–‹å§‹ï¼‰
    if 'model_trained' not in st.session_state:
        st.session_state.model_trained = False
    if 'fast_mode' not in st.session_state:
        st.session_state.fast_mode = True
    if 'classifier' not in st.session_state:
        st.session_state.classifier = None
    
    # è‡ªå®šç¾© CSS
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #FF6B6B;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sidebar-header {
        font-size: 1.5rem;
        color: #FF6B6B;
        margin-bottom: 1rem;
    }
    
    /* æ³¨æ„ï¼šä¸è¦éš±è—æ‰€æœ‰ selectboxï¼Œåªéš±è—ç‰¹å®šçš„å°èˆªç›¸é—œå…ƒç´  */
    
    /* éš±è—é é¢é ‚éƒ¨çš„å°èˆªæ¬„ */
    .css-1d391kg, .css-1rs6os, .css-17ziqus {
        display: none !important;
    }
    
    /* éš±è—å¤šé é¢æ‡‰ç”¨çš„é é¢é¸æ“‡å™¨ */
    .css-1kyxreq {
        display: none !important;
    }
    
    /* éš±è— Streamlit çš„é é¢å°èˆª */
    section[data-testid="stSidebar"] > div:first-child > div:first-child {
        display: none !important;
    }
    
    /* éš±è—å´é‚Šæ¬„é ‚éƒ¨çš„æ‡‰ç”¨ä¿¡æ¯ */
    .css-1lcbmhc, .css-1outpf7 {
        display: none !important;
    }
    
    /* æ›´ç²¾ç¢ºåœ°éš±è—å¤šé é¢å°èˆªå€åŸŸ */
    .stApp > header {
        display: none !important;
    }
    
    /* éš±è—å´é‚Šæ¬„ä¸­çš„é é¢å°èˆªæŒ‰éˆ• */
    .css-1544g2n, .css-1v0mbdj {
        display: none !important;
    }
    
    /* éš±è— Streamlit é»˜èªçš„é é¢æ¨™é¡Œå€åŸŸ */
    .css-18e3th9, .css-1d391kg {
        display: none !important;
    }
    
    /* éš±è—å¤šé é¢æ‡‰ç”¨çš„å°èˆªæ¬„ */
    div[data-testid="stSidebarNav"] {
        display: none !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ä¸»æ¨™é¡Œ
    st.markdown('<h1 class="main-header">ğŸ“§ åƒåœ¾éƒµä»¶åˆ†é¡ç³»çµ±</h1>', unsafe_allow_html=True)
    
    # åˆå§‹åŒ–åŸºç¤çµ„ä»¶
    data_loader = DataLoader()
    preprocessor = TextPreprocessor()
    visualizer = Visualizer()
    
    # å´é‚Šæ¬„é…ç½®
    st.sidebar.markdown('<h2 class="sidebar-header">ğŸ”§ ç³»çµ±é…ç½®</h2>', unsafe_allow_html=True)
    
    # é é¢é¸æ“‡
    page = st.sidebar.radio(
        "é¸æ“‡åŠŸèƒ½é é¢",
        ["ğŸ¯ Live Inference", "ğŸ“Š Model Performance", "ğŸ“ˆ Data Visualization"],
        help="é¸æ“‡è¦ä½¿ç”¨çš„åŠŸèƒ½é é¢"
    )
    
    # æ¨¡å‹é…ç½®å€åŸŸ
    st.sidebar.markdown("### æ¨¡å‹è¨­ç½®")
    
    # å¿«é€Ÿè¨“ç·´æ¨¡å¼ï¼ˆç§»åˆ°å¤–é¢ï¼‰
    fast_mode = st.sidebar.checkbox(
        "å¿«é€Ÿè¨“ç·´æ¨¡å¼",
        value=True,
        help="å•Ÿç”¨å¿«é€Ÿæ¨¡å¼å¯å¤§å¹…æ¸›å°‘è¨“ç·´æ™‚é–“ï¼Œä½†å¯èƒ½ç•¥å¾®é™ä½æº–ç¢ºç‡"
    )
    
    # æ•¸æ“šé›†å¤§å°é¸é …ï¼ˆç§»åˆ°å¤–é¢ï¼‰
    dataset_options = ["å®Œæ•´æ•¸æ“šé›†", "50% æ•¸æ“šé›†", "25% æ•¸æ“šé›†", "10% æ•¸æ“šé›†"]
    dataset_size = st.sidebar.selectbox(
        "æ•¸æ“šé›†å¤§å°",
        options=dataset_options,
        index=1 if fast_mode else 0,
        help="é¸æ“‡ç”¨æ–¼è¨“ç·´çš„æ•¸æ“šé›†å¤§å°ï¼Œè¼ƒå°çš„æ•¸æ“šé›†è¨“ç·´æ›´å¿«",
        key="dataset_size_selector"
    )
    
    # é«˜ç´šè¨­ç½®
    with st.sidebar.expander("é«˜ç´šè¨­ç½®"):
        # åˆ†é¡é–¾å€¼
        threshold = st.slider(
            "åˆ†é¡é–¾å€¼",
            min_value=0.1,
            max_value=0.9,
            value=0.5,
            step=0.05,
            help="èª¿æ•´åƒåœ¾éƒµä»¶åˆ†é¡çš„é–¾å€¼"
        )
        
        # ç‰¹å¾µæ•¸é‡
        max_features = st.slider(
            "æœ€å¤§ç‰¹å¾µæ•¸",
            min_value=1000,
            max_value=10000,
            value=3000 if fast_mode else 5000,
            step=500,
            help="TF-IDF å‘é‡åŒ–çš„æœ€å¤§ç‰¹å¾µæ•¸é‡"
        )
        
        # æ˜¯å¦ç§»é™¤åœç”¨è©
        remove_stopwords = st.checkbox(
            "ç§»é™¤åœç”¨è©",
            value=True,
            help="åœ¨é è™•ç†æ™‚æ˜¯å¦ç§»é™¤è‹±æ–‡åœç”¨è©"
        )
        
        # èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
        if st.checkbox("é¡¯ç¤ºèª¿è©¦ä¿¡æ¯", value=False, key="debug_info"):
            st.write(f"é¸æ“‡çš„æ•¸æ“šé›†å¤§å°: {dataset_size}")
            st.write(f"å¿«é€Ÿæ¨¡å¼: {fast_mode}")
            st.write(f"å¯ç”¨é¸é …: {dataset_options}")
    
    # æ ¹æ“šå¿«é€Ÿæ¨¡å¼è¨­ç½®åˆå§‹åŒ–åˆ†é¡å™¨
    if st.session_state.classifier is None or st.session_state.fast_mode != fast_mode:
        st.session_state.classifier = MultiModelClassifier(fast_mode=fast_mode)
        st.session_state.fast_mode = fast_mode
        st.session_state.model_trained = False  # é‡ç½®è¨“ç·´ç‹€æ…‹
    
    classifier = st.session_state.classifier
    
    # æ¨¡å‹é¸æ“‡
    if st.session_state.model_trained and hasattr(classifier, 'get_trained_models'):
        available_models = classifier.get_trained_models()
        if available_models:
            selected_model = st.sidebar.selectbox(
                "é¸æ“‡åˆ†é¡æ¨¡å‹",
                available_models,
                help="é¸æ“‡ç”¨æ–¼åˆ†é¡çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹"
            )
        else:
            st.sidebar.error("æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ¨¡å‹")
            selected_model = APP_CONFIG['model_names'][0]
    else:
        selected_model = st.sidebar.selectbox(
            "é¸æ“‡åˆ†é¡æ¨¡å‹",
            APP_CONFIG['model_names'],
            help="é¸æ“‡ç”¨æ–¼åˆ†é¡çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼ˆéœ€è¦å…ˆè¨“ç·´æ¨¡å‹ï¼‰",
            disabled=True
        )
    
    # æ•¸æ“šè¼‰å…¥ç‹€æ…‹
    st.sidebar.markdown("### ç³»çµ±ç‹€æ…‹")
    
    # è¼‰å…¥æ•¸æ“š
    with st.spinner("è¼‰å…¥æ•¸æ“šä¸­..."):
        data = data_loader.load_raw_dataset()
        dataset_info = data_loader.get_dataset_info()
    
    if not data.empty:
        st.sidebar.success("âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ")
        st.sidebar.info(f"ç¸½éƒµä»¶æ•¸: {dataset_info.get('total_messages', 0)}")
        st.sidebar.info(f"åƒåœ¾éƒµä»¶: {dataset_info.get('spam_count', 0)}")
        st.sidebar.info(f"æ­£å¸¸éƒµä»¶: {dataset_info.get('ham_count', 0)}")
    else:
        st.sidebar.error("âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—")
        st.error("ç„¡æ³•è¼‰å…¥æ•¸æ“šé›†ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
        return
    
    # è¨“ç·´æ¨¡å‹æŒ‰éˆ•
    train_button_col1, train_button_col2 = st.sidebar.columns([3, 1])
    
    with train_button_col1:
        train_clicked = st.button("ğŸš€ è¨“ç·´æ¨¡å‹", help="è¨“ç·´æ‰€æœ‰æ©Ÿå™¨å­¸ç¿’æ¨¡å‹")
    
    with train_button_col2:
        if fast_mode:
            st.caption("âš¡ å¿«é€Ÿæ¨¡å¼")
        else:
            st.caption("ğŸ¯ æ¨™æº–æ¨¡å¼")
    
    # é¡¯ç¤ºé ä¼°è¨“ç·´æ™‚é–“
    time_estimates = {
        ("å®Œæ•´æ•¸æ“šé›†", True): "1-2 åˆ†é˜",
        ("å®Œæ•´æ•¸æ“šé›†", False): "3-8 åˆ†é˜", 
        ("50% æ•¸æ“šé›†", True): "30-60 ç§’",
        ("50% æ•¸æ“šé›†", False): "1-3 åˆ†é˜",
        ("25% æ•¸æ“šé›†", True): "15-30 ç§’",
        ("25% æ•¸æ“šé›†", False): "30-90 ç§’",
        ("10% æ•¸æ“šé›†", True): "5-15 ç§’",
        ("10% æ•¸æ“šé›†", False): "15-30 ç§’"
    }
    
    estimated_time = time_estimates.get((dataset_size, fast_mode), "æœªçŸ¥")
    st.sidebar.info(f"â±ï¸ é ä¼°è¨“ç·´æ™‚é–“: {estimated_time}")
    
    if train_clicked:
        try:
            # æº–å‚™æ•¸æ“š
            with st.spinner("æº–å‚™æ•¸æ“šä¸­..."):
                train_df, test_df = data_loader.split_data()
                
                # æ ¹æ“šé¸æ“‡çš„æ•¸æ“šé›†å¤§å°é€²è¡Œæ¡æ¨£
                if dataset_size != "å®Œæ•´æ•¸æ“šé›†":
                    size_map = {"50% æ•¸æ“šé›†": 0.5, "25% æ•¸æ“šé›†": 0.25, "10% æ•¸æ“šé›†": 0.1}
                    sample_ratio = size_map[dataset_size]
                    
                    train_df = train_df.sample(frac=sample_ratio, random_state=42)
                    test_df = test_df.sample(frac=sample_ratio, random_state=42)
                    
                    st.sidebar.info(f"ä½¿ç”¨ {len(train_df)} å€‹è¨“ç·´æ¨£æœ¬ï¼Œ{len(test_df)} å€‹æ¸¬è©¦æ¨£æœ¬")
                
                # é è™•ç†æ–‡æœ¬
                train_texts = [preprocessor.preprocess_text(text, remove_stopwords) 
                              for text in train_df['text']]
                test_texts = [preprocessor.preprocess_text(text, remove_stopwords) 
                             for text in test_df['text']]
                
                # è¨“ç·´å‘é‡åŒ–å™¨
                preprocessor.fit_vectorizers(train_texts, max_features)
                
                # æå–ç‰¹å¾µ
                X_train = preprocessor.extract_tfidf_features(train_texts)
                X_test = preprocessor.extract_tfidf_features(test_texts)
                
                y_train = train_df['label'].values
                y_test = test_df['label'].values
            
            st.sidebar.success("âœ… æ•¸æ“šæº–å‚™å®Œæˆ")
            
            # è¨“ç·´æ¨¡å‹
            classifier.preprocessor = preprocessor
            classifier.train_models(X_train, y_train, X_test, y_test)
            
            # ä¿å­˜æ¨¡å‹
            classifier.save_models()
            
            st.session_state.model_trained = True
            st.sidebar.success("âœ… æ¨¡å‹è¨“ç·´å®Œæˆ")
            
        except Exception as e:
            st.sidebar.error(f"âŒ æ¨¡å‹è¨“ç·´å¤±æ•—: {str(e)}")
            st.error(f"è©³ç´°éŒ¯èª¤ä¿¡æ¯: {str(e)}")
    
    # å˜—è©¦è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹
    if not st.session_state.model_trained:
        if classifier.load_models():
            st.session_state.model_trained = True
            st.sidebar.success("âœ… å·²è¼‰å…¥é è¨“ç·´æ¨¡å‹")
    
    # é¡¯ç¤ºæ¨¡å‹ç‹€æ…‹
    if st.session_state.model_trained:
        st.sidebar.success("ğŸ¤– æ¨¡å‹å·²å°±ç·’")
        
        # é¡¯ç¤ºå·²è¨“ç·´çš„æ¨¡å‹
        if hasattr(classifier, 'get_trained_models'):
            trained_models = classifier.get_trained_models()
            model_status = classifier.get_model_status()
            
            with st.sidebar.expander("æ¨¡å‹ç‹€æ…‹è©³æƒ…"):
                for model_name, is_trained in model_status.items():
                    if is_trained:
                        st.success(f"âœ… {model_name}")
                    else:
                        st.error(f"âŒ {model_name}")
                
                st.write(f"**å¯ç”¨æ¨¡å‹**: {len(trained_models)}/{len(APP_CONFIG['model_names'])}")
    else:
        st.sidebar.warning("âš ï¸ è«‹å…ˆè¨“ç·´æ¨¡å‹")
    
    # é é¢è·¯ç”±
    if page == "ğŸ¯ Live Inference":
        live_inference.show_page(
            classifier=classifier,
            data_loader=data_loader,
            visualizer=visualizer,
            selected_model=selected_model,
            threshold=threshold,
            model_trained=st.session_state.model_trained
        )
    elif page == "ğŸ“Š Model Performance":
        model_performance.show_page(
            classifier=classifier,
            visualizer=visualizer,
            model_trained=st.session_state.model_trained
        )
    elif page == "ğŸ“ˆ Data Visualization":
        data_visualization.show_page(
            data=data,
            data_loader=data_loader,
            preprocessor=preprocessor,
            visualizer=visualizer
        )
    
    # é è…³
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 1rem;'>
            <p>åƒåœ¾éƒµä»¶åˆ†é¡ç³»çµ± - Phase 1 | 
            åŸºæ–¼ Streamlit å’Œ scikit-learn æ§‹å»º | 
            Â© 2025</p>
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()